Preprocessing: We separate numerical and categorical features, applying scaling to the numerical features and one-hot encoding to the categorical ones.

Train-Test Split: We split the data into training (70%) and testing (30%) sets.

Model Training: We fit a Logistic Regression model with balanced class weights to handle any potential class imbalance.

Model Evaluation: We print out key evaluation metrics such as classification report, ROC-AUC score, and confusion matrix.

Saving the Model: You can save the trained model as a .pkl file using joblib for future use.

To enhance your Logistic Regression code by including computational overhead metrics and generating graphs/charts, Iâ€™ll provide you with updated code. Here's the breakdown of what we'll add:

Measure Computational Overhead:

Time: Measure the time taken for training and prediction using the time module.
Memory: Measure the memory usage using the memory_profiler library.
Generate Graphs and Charts:

Confusion Matrix Visualization: Use matplotlib and seaborn to plot the confusion matrix.
ROC Curve: Plot the ROC curve using matplotlib.